{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdsOn-49W0-W",
        "outputId": "809370ce-072c-4aff-8336-139a941b21a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install numpy\n",
        "!pip install Pillow\n",
        "!pip install petastorm\n",
        "!pip install hyperopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-96P3VFYXIMU",
        "outputId": "ca5f4a3c-4552-4847-8615-691d7c0ce20d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=9df92f869bf78bfebbff08a9c1feb9ce338c606b5553e06ac8fb61517d504108\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting petastorm\n",
            "  Downloading petastorm-0.12.1-py2.py3-none-any.whl (284 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.2.1 (from petastorm)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: diskcache>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (5.6.3)\n",
            "Requirement already satisfied: future>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from petastorm) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from petastorm) (1.23.5)\n",
            "Requirement already satisfied: packaging>=15.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (23.2)\n",
            "Requirement already satisfied: pandas>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (1.5.3)\n",
            "Requirement already satisfied: psutil>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (5.9.5)\n",
            "Requirement already satisfied: pyspark>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (3.5.0)\n",
            "Requirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (23.2.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from petastorm) (10.0.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from petastorm) (2023.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.0->petastorm) (2023.3.post1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark>=2.1.0->petastorm) (0.10.9.7)\n",
            "Installing collected packages: dill, petastorm\n",
            "Successfully installed dill-0.3.7 petastorm-0.12.1\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "UIXa2R09YvI9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
        "from petastorm.etl.dataset_metadata import materialize_dataset\n",
        "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField"
      ],
      "metadata": {
        "id": "p5cLrVj_Y0v5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm import TransformSpec"
      ],
      "metadata": {
        "id": "7aTlNfi6ZHqT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "Uz_tecpFZPKa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (ArrayType, BinaryType, BooleanType, ByteType, DoubleType, FloatType,\n",
        "                               IntegerType, LongType, ShortType, StringType, StructField, StructType)"
      ],
      "metadata": {
        "id": "RX6Fi3cXZU6R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Create petastorm store\").config(\"spark.memory.offHeap.enabled\", True) \\\n",
        "                            .config(\"spark.memory.offHeap.size\", \"30g\").getOrCreate()\n",
        "\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'file:/content/drive')"
      ],
      "metadata": {
        "id": "fwW6MJRgZkYi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand Primitives"
      ],
      "metadata": {
        "id": "W8guOolgaHrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"bool_col\", BooleanType(), False),\n",
        "    StructField(\"float_col\", FloatType(), False),\n",
        "    StructField(\"double_col\", DoubleType(), False),\n",
        "    StructField(\"short_col\", ShortType(), False),\n",
        "    StructField(\"int_col\", IntegerType(), False),\n",
        "    StructField(\"long_col\", LongType(), False),\n",
        "    StructField(\"str_col\", StringType(), False),\n",
        "    StructField(\"bin_col\", BinaryType(), False),\n",
        "    StructField(\"byte_col\", ByteType(), False),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "JIzqln9waDNb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(\n",
        "    [(True, 0.12, 432.1, 5, 5, 0, \"hello\", bytearray(b\"spark\\x01\\x02\"), -128),\n",
        "    (False, 123.45, 0.987, 9, 908, 765, \"petastorm\", bytearray(b\"\\x0012345\"), 127)],\n",
        "     schema=schema).coalesce(1)\n",
        "\n",
        "# if we use numPartition > 1 in coalesce, the order of the loaded dataset would be non-deteministic\n",
        "# just for the learning phase - don't use in production\n",
        "expected_df = df.collect()"
      ],
      "metadata": {
        "id": "fAj_jJGhdUXD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eec1nHEMeICd",
        "outputId": "373dd9ca-465f-4a66-e3a6-7b9110df3856"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(bool_col=True, float_col=0.11999999731779099, double_col=432.1, short_col=5, int_col=5, long_col=0, str_col='hello', bin_col=bytearray(b'spark\\x01\\x02'), byte_col=-128),\n",
              " Row(bool_col=False, float_col=123.44999694824219, double_col=0.987, short_col=9, int_col=908, long_col=765, str_col='petastorm', bin_col=bytearray(b'\\x0012345'), byte_col=127)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Tensorflow dataset"
      ],
      "metadata": {
        "id": "-fWnDIJWefJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UBnSO3rfed8D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "converter = make_spark_converter(df)\n",
        "with converter.make_tf_dataset() as dataset:\n",
        "        iterator = iter(dataset)\n",
        "        print(iterator.element_spec)\n",
        "\n",
        "        tensor = iterator.get_next()\n",
        "        print(tensor)"
      ],
      "metadata": {
        "id": "uRkjwoTGek8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJpXgmDoeyir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}