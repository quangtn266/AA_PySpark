{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cecd0d-77d2-415a-bd01-0ea8281d3a95",
   "metadata": {},
   "source": [
    "# Working with Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd84da6-e8f9-40f6-9293-e57e3e3deb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0267c9-9399-4dd6-92c3-d2c99dd2242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/13 20:12:41 WARN Utils: Your hostname, quangtn933.local resolves to a loopback address: 127.0.0.1; using 192.168.1.90 instead (on interface en0)\n",
      "24/01/13 20:12:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/13 20:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName(\"Into\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443cb1b4-10d9-430b-84d0-3fdb9007abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45882a5-47ec-4348-8228-7956ddb8e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db608afa-19a5-4089-a220-962a03890dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec_b8bd6d710576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25c2122-d3e8-46da-9a7d-6438649ec61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 13:25:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/01/12 13:25:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: array<string>, result: vector]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98628e51-7f3f-4cb7-83bc-0311b3b5b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.046886457502841955,-0.07654499709606172,-0.008032251521945]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.017943344394942478,-0.0315708705623235,-0.009133852552622557]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.016518119536340237,-0.009977189078927041,-0.005242400616407395]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec964fe-e9c9-48db-8769-a9a186a33df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0e6ef4-36ce-421d-ad3b-67b44931cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi|I|heard|about|Spark\"),\n",
    "    (1, \"I     wish Java     could  use case   classes\"),\n",
    "    (2, \"Logistic, regression, model,are,neat\")],\n",
    "    [\"id\", \"sentence\"]                                          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bb2463a-c097-477c-9a0b-8f7ffc8333d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "587f8f3f-7347-46fd-8443-73097802431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+----------------------------------------------------------------+------+\n",
      "|sentence                                     |words                                                           |tokens|\n",
      "+---------------------------------------------+----------------------------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                       |[hi|i|heard|about|spark]                                        |1     |\n",
      "|I     wish Java     could  use case   classes|[i, , , , , wish, java, , , , , could, , use, case, , , classes]|18    |\n",
      "|Logistic, regression, model,are,neat         |[logistic,, regression,, model,are,neat]                        |3     |\n",
      "+---------------------------------------------+----------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "050512e0-d7a0-4b06-97ad-6cdaed17d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+------------------------------------------+------+\n",
      "|sentence                                     |words                                     |tokens|\n",
      "+---------------------------------------------+------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                       |[hi, i, heard, about, spark]              |5     |\n",
      "|I     wish Java     could  use case   classes|[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic, regression, model,are,neat         |[logistic, regression, model, are, neat]  |5     |\n",
      "+---------------------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1380372f-248f-4886-8327-e91db06ec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c708b5be-7d06-4048-983d-24a6148bba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"ballon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d016e96-b4a8-4ada-a40b-d1925c52cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 14:29:33 WARN StopWordsRemover: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, ballon]  |[saw, red, ballon]  |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4edaf-548d-4a95-bc42-16404eb41558",
   "metadata": {},
   "source": [
    "## Code Example >> Tokenizer >> N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f71784f-9a1f-47e4-bc89-630541dca86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69c2c70d-e226-4379-b2bd-ccf66824c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish, wish Java, Java could\"),\n",
    "    (2, \"Logistic regression, regression models\")],\n",
    "            [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14504c57-6cbc-464f-a9e8-b04e393f5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "wordDataFrame = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf2c91a-9ccc-4047-88e1-d75df8d0cc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, words: array<string>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f428282a-91df-445d-b791-a3bbaa1363d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e07510b-deef-421d-b0e4-75f31e2a31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+\n",
      "|ngrams                                                           |\n",
      "+-----------------------------------------------------------------+\n",
      "|[hi i, i heard, heard about, about spark]                        |\n",
      "|[i wish,, wish, wish, wish java,, java, java, java could]        |\n",
      "|[logistic regression,, regression, regression, regression models]|\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f902c656-2f78-4c37-a815-127556eefca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 5.1),\n",
    "    (1, 5.8),\n",
    "    (2, 0.2)],\n",
    "    [\"id\", \"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12ab38e9-fa89-4275-b63d-d89e5c247d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e40f913e-f0a2-49cd-b17a-9e8c07a2a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    5.1|              1.0|\n",
      "|  1|    5.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f2affa3-a53c-4405-ace1-431b3b69703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7ba98c0-a520-407b-a806-172185278ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f54a639-f895-4391-84d6-ed9b24dadf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f011e6be-d3cf-455b-8aa5-d439069d260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 15:05:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6ee69b5-4a84-44c7-a842-79bf03383b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|pcaFeatures                                                 |\n",
      "+------------------------------------------------------------+\n",
      "|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n",
      "|[-4.645104331781533,-1.1167972663619048,-1.0091435193998504]|\n",
      "|[-6.428880535676488,-5.337951427775359,-1.0091435193998508] |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5826ad5-35a7-40b8-83a0-71b16e8b581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cf9e45b-0928-4a49-a147-6282cc307548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),),],\n",
    "    [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9847ed4-8a88-46ac-9c89-db1baae1ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39c59233-6c23-4a5e-9503-0f71df2e1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------------------+\n",
      "|features |polyFeatures                                                                        |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "|[2.0,1.0]|[2.0,4.0,8.0,16.0,32.0,1.0,2.0,4.0,8.0,16.0,1.0,2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]|\n",
      "|[0.0,0.0]|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polyDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d283384-1b8c-4847-9f51-737e02dce255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6053078c-d127-4400-bb77-085d9f155f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|featuresDCT                                                     |\n",
      "+----------------------------------------------------------------+\n",
      "|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|\n",
      "|[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |\n",
      "|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]), ),\n",
    "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]), ),\n",
    "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
    "\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
    "dctDf = dct.transform(df)\n",
    "dctDf.select(\"featuresDCT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3815e4e7-d75f-4785-98b5-5e143674afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c84329c-8d15-4c56-b3af-ab160a8c2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0, 200]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0, 2]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0, 0]),)],\n",
    "    [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97695676-1bdf-4c06-8fca-7bdb13ee266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17b1119d-d1f5-4ef5-b2e8-eecb7c4f6cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1]\n",
    "scaledData = scalerModel.transform(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21bbcf13-615f-418a-895b-6afa7e415453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[1.0,0.1,-8.0,200.0]|[0.25,0.010000000...|\n",
      "|  [2.0,1.0,-4.0,2.0]| [0.5,0.1,-0.5,0.01]|\n",
      "|  [4.0,10.0,8.0,0.0]|   [1.0,1.0,1.0,0.0]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bcda76b-0ef3-4b27-9f47-c3e7bb33b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6818be31-3794-44b9-a4b2-b73a796ca913",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e48be6b9-75f0-4540-82b8-f75b1b442f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a285caeb-bfed-4158-b321-120c934c6679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bucketizer_6d5e399063e1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d9adbc3-71ef-449c-adb6-4ed18359169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucktedData = bucketizer.transform(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c66b393-e7d7-4579-839a-ebaf90d2a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb67ac4a-d54f-47b6-b226-69c12040b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucktedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4dd6660-4444-4c3a-88c7-e797b0f5d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c864e681-6f56-4cfa-ac83-d05501f99f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some Vector data; also works for sparse vectors\n",
    "data = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4deb0f21-e884-4aad-95d9-ec136808eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|       vector|transformedVector|\n",
      "+-------------+-----------------+\n",
      "|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, [\"vector\"])\n",
    "transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),\n",
    "                                inputCol=\"vector\", outputCol=\"transformedVector\")\n",
    "# Batch transform the vectors to create new column:\n",
    "transformer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2f09bd-541f-4e2d-a33a-0316ffedce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbbe444-e82e-4209-bed8-05b5f43ff782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer_57304a316422"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/13 15:30:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd86b389-86dc-487f-9b4b-5308d7a45849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f45d91-4ac2-4de6-9cb8-1a56c1439892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data_frame = spark.createDataFrame([\n",
    "    (0, \"Hi i think pyspark is cool\", \"happy\"),\n",
    "    (1, \"All I want is a pyspark cluster\", \"indifferent\"),\n",
    "    (2, \"I finally understand how ML works\", \"Fulfilled\"),\n",
    "    (3, \"Yet another sentence about pyspark and ML\", \"indifferent\"),\n",
    "    (4, \"Why didn't I know aboyt mllib before\", \"sad\"),\n",
    "    (5, \"Yes, I can\", \"happy\")],\n",
    "    [\"id\", \"sentence\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d8157a-59ca-46de-a891-a8c312c41a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, sentiment: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc6e8b3-2d77-4c3d-bc21-e77ba20cca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ef60bd-74b1-467d-9e5c-8367fb8e10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575ecd90-3f72-4c81-9577-2bc4b5b25bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sentence_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb53850-f745-41f8-ae67-9832c030a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|id |sentence                                 |sentiment  |words                                            |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|0  |Hi i think pyspark is cool               |happy      |[hi, i, think, pyspark, is, cool]                |\n",
      "|1  |All I want is a pyspark cluster          |indifferent|[all, i, want, is, a, pyspark, cluster]          |\n",
      "|2  |I finally understand how ML works        |Fulfilled  |[i, finally, understand, how, ml, works]         |\n",
      "|3  |Yet another sentence about pyspark and ML|indifferent|[yet, another, sentence, about, pyspark, and, ml]|\n",
      "|4  |Why didn't I know aboyt mllib before     |sad        |[why, didn't, i, know, aboyt, mllib, before]     |\n",
      "|5  |Yes, I can                               |happy      |[yes,, i, can]                                   |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03dba1cb-da6d-478d-b6c7-1a9784abe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f241f5-4073-4dd3-ace1-e1c711c8c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/13 15:47:31 WARN StopWordsRemover: Default locale set was [en_VN]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------------------+\n",
      "|words                                            |meaningful_words                     |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "|[hi, i, think, pyspark, is, cool]                |[hi, think, pyspark, cool]           |\n",
      "|[all, i, want, is, a, pyspark, cluster]          |[want, pyspark, cluster]             |\n",
      "|[i, finally, understand, how, ml, works]         |[finally, understand, ml, works]     |\n",
      "|[yet, another, sentence, about, pyspark, and, ml]|[yet, another, sentence, pyspark, ml]|\n",
      "|[why, didn't, i, know, aboyt, mllib, before]     |[know, aboyt, mllib]                 |\n",
      "|[yes,, i, can]                                   |[yes,]                               |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"meaningful_words\")\n",
    "meaningful_data_frame = remover.transform(tokenized)\n",
    "\n",
    "meaningful_data_frame.select(\"words\", \"meaningful_words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64228852-f856-4a5e-92c1-e43da45af635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "| id|            sentence|  sentiment|               words|    meaningful_words|categoryIndex|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "|  0|Hi i think pyspar...|      happy|[hi, i, think, py...|[hi, think, pyspa...|          0.0|\n",
      "|  1|All I want is a p...|indifferent|[all, i, want, is...|[want, pyspark, c...|          1.0|\n",
      "|  2|I finally underst...|  Fulfilled|[i, finally, unde...|[finally, underst...|          2.0|\n",
      "|  3|Yet another sente...|indifferent|[yet, another, se...|[yet, another, se...|          1.0|\n",
      "|  4|Why didn't I know...|        sad|[why, didn't, i, ...|[know, aboyt, mllib]|          3.0|\n",
      "|  5|          Yes, I can|      happy|      [yes,, i, can]|              [yes,]|          0.0|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(meaningful_data_frame).transform(meaningful_data_frame)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5f2558f-85e5-480b-bdce-127c36d4eca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, sentiment: string, words: array<string>, meaningful_words: array<string>, categoryIndex: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b722e5b2-aaef-4a9a-afd6-8b8b533be8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+---------+----+\n",
      "|sentence_id|happy|indifferent|Fulfilled| sad|\n",
      "+-----------+-----+-----------+---------+----+\n",
      "|          0| 0.01|       0.43|      0.3| 0.5|\n",
      "|          1|0.097|       0.21|      0.2| 0.9|\n",
      "|          2|  0.4|      0.329|     0.97| 0.4|\n",
      "|          3|  0.7|        0.4|      0.3|0.87|\n",
      "|          4| 0.34|        0.4|      0.3|0.78|\n",
      "|          5|  0.1|        0.3|     0.31|0.29|\n",
      "+-----------+-----+-----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_frame = spark.createDataFrame([\n",
    "    (0, 0.01, 0.43, 0.3, 0.5),\n",
    "    (1, 0.097, 0.21, 0.2, 0.9),\n",
    "    (2, 0.4, 0.329, 0.97, 0.4),\n",
    "    (3, 0.7, 0.4, 0.3, 0.87),\n",
    "    (4, 0.34, 0.4, 0.3, 0.78),\n",
    "    (5, 0.1, 0.3, 0.31, 0.29)],\n",
    "    [\"sentence_id\", \"happy\", \"indifferent\", \"Fulfilled\", \"sad\"])\n",
    "\n",
    "sentiment_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5b5d89d-7040-47f8-9289-b2e63257ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|happy|\n",
      "+-----+\n",
      "| 0.01|\n",
      "|0.097|\n",
      "|  0.4|\n",
      "|  0.7|\n",
      "| 0.34|\n",
      "|  0.1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casted_data_frame = sentiment_data_frame.selectExpr(\"cast(happy as double)\")\n",
    "casted_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56c7053f-aa93-45e6-8af6-2349ab0b4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentence_id: long (nullable = true)\n",
      " |-- happy: double (nullable = true)\n",
      " |-- indifferent: double (nullable = true)\n",
      " |-- Fulfilled: double (nullable = true)\n",
      " |-- sad: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f329d8-ba8f-427b-bd19-f19d1f42b97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, sentence: string, words: array<string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/13 20:12:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi, I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "wordsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4d8091-5d58-4e92-8d85-8975ed200867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi, I heard about...|[hi,, i, heard, a...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa63cd5f-01fc-48f0-83e3-eb403719010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",\n",
    "                      numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc9a0a0-1c5d-468d-a2f5-ff820e4dd412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, sentence: string, words: array<string>, rawFeatures: vector]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1dd108-c5ba-48e7-8a1b-a14ced89d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi, I heard about...|[hi,, i, heard, a...|(20,[6,11,13,16],...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b397caf-c5a2-497f-bffa-391a4b64c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi, I heard about...|[hi,, i, heard, a...|(20,[6,11,13,16],...|(20,[6,11,13,16],...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bb047-503c-4af4-901c-a7ea71d8ac42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
