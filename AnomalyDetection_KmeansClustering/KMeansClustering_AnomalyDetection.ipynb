{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f9594e-13af-455b-8959-092c9068b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27647e0-c841-4f92-bfa6-1e96f3632f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/31 14:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/31 14:41:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"6g\").appName('chapter5').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634f706-3e98-4637-96d2-7e5ef9d79dca",
   "metadata": {},
   "source": [
    "## Indetify Anamolous Network Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6692422f-2eaa-429f-a3d5-00ef6bf8ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"/Users/quangtn/Desktop/01_work/01_job/02_ml/PySpark/chapter5/kddcup.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e901207c-5c91-497c-ba0f-880e2b65e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [  \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "  \"dst_host_count\", \"dst_host_srv_count\",\n",
    "  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "  \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c49695-d602-44d3-8dce-9f7cacf13dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b9e6b2-1060-4ea6-976b-3ecedf0e3a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|           label|  count|\n",
      "+----------------+-------+\n",
      "|          smurf.|2807886|\n",
      "|        neptune.|1072017|\n",
      "|         normal.| 972781|\n",
      "|          satan.|  15892|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|           nmap.|   2316|\n",
      "|           back.|   2203|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|            pod.|    264|\n",
      "|   guess_passwd.|     53|\n",
      "|buffer_overflow.|     30|\n",
      "|           land.|     21|\n",
      "|    warezmaster.|     20|\n",
      "|           imap.|     12|\n",
      "|        rootkit.|     10|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|       multihop.|      7|\n",
      "|            phf.|      4|\n",
      "|           perl.|      3|\n",
      "|            spy.|      2|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a519592d-35af-4792-8320-89682876db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 14:59:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a0d375-9e22-4bd5-b507-60318ca87a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d9f6bb-df54-4b55-b33f-d661250ab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601995e6-63ee-4554-ae62-03832b6af8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 15:02:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/31 15:02:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17f7390e-7d91-4775-868c-f63b19923f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = pipeline_model.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ef8348-bafe-4139-883b-00aaf99c8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4.83401949e+01, 1.83462155e+03, 8.26203190e+02, 5.71611720e-06,\n",
      "       6.48779303e-04, 7.96173468e-06, 1.24376586e-02, 3.20510858e-05,\n",
      "       1.43529049e-01, 8.08830584e-03, 6.81851124e-05, 3.67464677e-05,\n",
      "       1.29349608e-02, 1.18874823e-03, 7.43095237e-05, 1.02114351e-03,\n",
      "       0.00000000e+00, 4.08294086e-07, 8.35165553e-04, 3.34973508e+02,\n",
      "       2.95267146e+02, 1.77970317e-01, 1.78036989e-01, 5.76648988e-02,\n",
      "       5.77299094e-02, 7.89884132e-01, 2.11796106e-02, 2.82608101e-02,\n",
      "       2.32981078e+02, 1.89214283e+02, 7.53713390e-01, 3.07109788e-02,\n",
      "       6.05051931e-01, 6.46410789e-03, 1.78091184e-01, 1.77885898e-01,\n",
      "       5.79276115e-02, 5.76592214e-02]),\n",
      " array([1.0999000e+04, 0.0000000e+00, 1.3099374e+09, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       2.5500000e+02, 1.0000000e+00, 0.0000000e+00, 6.5000000e-01,\n",
      "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00])]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cbf3709-bcc1-4f02-b982-2ae2804fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cluster = pipeline_model.transform(numeric_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d25d19c-115e-46e0-85f1-fd9bc18cc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------+\n",
      "|cluster|           label|  count|\n",
      "+-------+----------------+-------+\n",
      "|      0|          smurf.|2807886|\n",
      "|      0|        neptune.|1072017|\n",
      "|      0|         normal.| 972781|\n",
      "|      0|          satan.|  15892|\n",
      "|      0|        ipsweep.|  12481|\n",
      "|      0|      portsweep.|  10412|\n",
      "|      0|           nmap.|   2316|\n",
      "|      0|           back.|   2203|\n",
      "|      0|    warezclient.|   1020|\n",
      "|      0|       teardrop.|    979|\n",
      "|      0|            pod.|    264|\n",
      "|      0|   guess_passwd.|     53|\n",
      "|      0|buffer_overflow.|     30|\n",
      "|      0|           land.|     21|\n",
      "|      0|    warezmaster.|     20|\n",
      "|      0|           imap.|     12|\n",
      "|      0|        rootkit.|     10|\n",
      "|      0|     loadmodule.|      9|\n",
      "|      0|      ftp_write.|      8|\n",
      "|      0|       multihop.|      7|\n",
      "|      0|            phf.|      4|\n",
      "|      0|           perl.|      3|\n",
      "|      0|            spy.|      2|\n",
      "|      1|      portsweep.|      1|\n",
      "+-------+----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aee51b6b-dced-430c-bf5e-2d1b3cc38177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data, k):\n",
    "  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "  kmeans = KMeans().setSeed(randint(100, 10000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "  pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "  pipeline_model = pipeline.fit(input_numeric_only)\n",
    "\n",
    "  evaluator = ClusteringEvaluator(predictionCol='cluster', featuresCol=\"featureVector\")\n",
    "  predictions = pipeline_model.transform(numeric_only)\n",
    "  score = evaluator.evaluate(predictions)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b08c718-10d3-4b9f-90b1-dfde4ed0b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996553386887673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for k in list(range(20, 40, 20)):\n",
    "  print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09aa05fa-1368-4b97-bc42-540092672bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100, 100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "\n",
    "    evaluator = ClusteringEvaluator(predictionCol='cluster', featuresCol=\"featureVector\")\n",
    "    predictions = pipeline_model.transform(numeric_only)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "651a3227-b35e-4506-b52a-ee0249380057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.9997630974591062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0.9997063047834809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for k in list(range(20,41,20)):\n",
    "    print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea7926f8-8abf-4f6b-8307-c4ee763d8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37915286-7efa-45a8-941b-dcdc7315a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_2(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100, 10000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "\n",
    "    evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\")\n",
    "    predictions = pipeline_model.transform(numeric_only)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37a6c275-5800-40d7-947f-162c73b8ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 0.8414967421804316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 347:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 0.9018280253834207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for k in list(range(60,81,20)):\n",
    "    print(k, clustering_score_2(numeric_only,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26f7edfa-93f5-4996-a6c9-78b04e3ae941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def one_hot_pipeline(input_col):\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"-_indexed\")\n",
    "    encoder = OneHotEncoder().setInputCol(input_col).setOutputCol(input_col + \"_vec\")\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return pipeline, input_col +\"_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32e61d74-7b03-4f47-bb65-1932c648c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_3(input_data, k):\n",
    "    proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "    service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
    "    flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "\n",
    "    assemble_cols = set(input_data.columns) - {\"label\", \"protocol_type\", \"service\" , \"flag\"} | \\\n",
    "                {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "\n",
    "    assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100, 100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_data)\n",
    "\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeas_model.summary.trainingCost\n",
    "    return training_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c3bdecc-3d90-4997-aa1b-67c3e98e3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k in list(range(60, 81, 20)):\n",
    "#    print(k, clustering_score_3(data, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddddea1a-d0e3-4a52-bd13-f1d151ffcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1da53211-a64d-4d6b-b7bc-3a9abf68fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    values = [c for c in counts if (c>0)]\n",
    "    n = sum(values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7e95310-1940-4b16-8911-8da6cc428023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b285c5ab-9ae1-43d7-a224-e7f1427f612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb0b9fe5-c318-4b62-8107-3f385b2528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c39b6b71-99ac-4cd2-bc63-d6d6d3867fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"cluster\")\n",
    "p_col = df['count']/fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "result = with_p_col.groupBy(\"cluster\").agg(fun.sum(col(\"p_col\")*fun.log2(col(\"p_col\"))).alias(\"entropy\"), fun.sum(col(\"count\")).alias(\"cluster_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03c03689-2b39-4f4d-90eb-cff05f481dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.withColumn('weightedClusterEntropy', col('entropy')*col('cluster_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "715736b4-563d-4c7c-9ccd-f3f1584adc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "933431ad-4f29-4231-b907-cec9fd372411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.4873454858776902"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_cluster_entropy_avg[0][0]/data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f64dd83e-6883-492e-8104-419aaed7de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "    (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "    (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "    (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "\n",
    "    assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "    assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "\n",
    "    kmeans = KMeans(seed=randint(100, 10000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "    pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    return pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f251fca6-cbc6-449c-b1f1-692597976ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n",
    "count_by_cluster_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd31b1c1-97c2-4560-9091-c685d0e6a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd624ebb-00f6-4c98-b9aa-2cf95e5458d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_model = pipeline_model.stages[-1]\n",
    "centroids = k_means_model.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ff7cb95-7a0e-46e8-9e51-581e62bada74",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = pipeline_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04df5c12-21b7-4f2d-8478-3f3a200e1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_func(cluster, vec):\n",
    "    return float(np.linalg.norm(centroids[cluster] - vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fec12bca-b337-4625-b563-a5f89f724add",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `scaledFeatureVector` cannot be resolved. Did you mean one of the following? [`featureVector`, `cluster`, `same_srv_rate`, `urgent`, `count`].;\n'Project [cluster#40343, 'scaledFeatureVector]\n+- Project [duration#101, protocol_type#102, service#103, flag#104, src_bytes#105, dst_bytes#106, land#107, wrong_fragment#108, urgent#109, hot#110, num_failed_logins#111, logged_in#112, num_compromised#113, root_shell#114, su_attempted#115, num_root#116, num_file_creations#117, num_shells#118, num_access_files#119, num_outbound_cmds#120, is_host_login#121, is_guest_login#122, count#123, srv_count#124, ... 20 more fields]\n   +- Project [duration#101, protocol_type#102, service#103, flag#104, src_bytes#105, dst_bytes#106, land#107, wrong_fragment#108, urgent#109, hot#110, num_failed_logins#111, logged_in#112, num_compromised#113, root_shell#114, su_attempted#115, num_root#116, num_file_creations#117, num_shells#118, num_access_files#119, num_outbound_cmds#120, is_host_login#121, is_guest_login#122, count#123, srv_count#124, ... 19 more fields]\n      +- Project [_c0#17 AS duration#101, _c1#18 AS protocol_type#102, _c2#19 AS service#103, _c3#20 AS flag#104, _c4#21 AS src_bytes#105, _c5#22 AS dst_bytes#106, _c6#23 AS land#107, _c7#24 AS wrong_fragment#108, _c8#25 AS urgent#109, _c9#26 AS hot#110, _c10#27 AS num_failed_logins#111, _c11#28 AS logged_in#112, _c12#29 AS num_compromised#113, _c13#30 AS root_shell#114, _c14#31 AS su_attempted#115, _c15#32 AS num_root#116, _c16#33 AS num_file_creations#117, _c17#34 AS num_shells#118, _c18#35 AS num_access_files#119, _c19#36 AS num_outbound_cmds#120, _c20#37 AS is_host_login#121, _c21#38 AS is_guest_login#122, _c22#39 AS count#123, _c23#40 AS srv_count#124, ... 18 more fields]\n         +- Relation [_c0#17,_c1#18,_c2#19,_c3#20,_c4#21,_c5#22,_c6#23,_c7#24,_c8#25,_c9#26,_c10#27,_c11#28,_c12#29,_c13#30,_c14#31,_c15#32,_c16#33,_c17#34,_c18#35,_c19#36,_c20#37,_c21#38,_c22#39,_c23#40,... 18 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[43mclustered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscaledFeatureVector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaledFeatureVector\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bentoML/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bentoML/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bentoML/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `scaledFeatureVector` cannot be resolved. Did you mean one of the following? [`featureVector`, `cluster`, `same_srv_rate`, `urgent`, `count`].;\n'Project [cluster#40343, 'scaledFeatureVector]\n+- Project [duration#101, protocol_type#102, service#103, flag#104, src_bytes#105, dst_bytes#106, land#107, wrong_fragment#108, urgent#109, hot#110, num_failed_logins#111, logged_in#112, num_compromised#113, root_shell#114, su_attempted#115, num_root#116, num_file_creations#117, num_shells#118, num_access_files#119, num_outbound_cmds#120, is_host_login#121, is_guest_login#122, count#123, srv_count#124, ... 20 more fields]\n   +- Project [duration#101, protocol_type#102, service#103, flag#104, src_bytes#105, dst_bytes#106, land#107, wrong_fragment#108, urgent#109, hot#110, num_failed_logins#111, logged_in#112, num_compromised#113, root_shell#114, su_attempted#115, num_root#116, num_file_creations#117, num_shells#118, num_access_files#119, num_outbound_cmds#120, is_host_login#121, is_guest_login#122, count#123, srv_count#124, ... 19 more fields]\n      +- Project [_c0#17 AS duration#101, _c1#18 AS protocol_type#102, _c2#19 AS service#103, _c3#20 AS flag#104, _c4#21 AS src_bytes#105, _c5#22 AS dst_bytes#106, _c6#23 AS land#107, _c7#24 AS wrong_fragment#108, _c8#25 AS urgent#109, _c9#26 AS hot#110, _c10#27 AS num_failed_logins#111, _c11#28 AS logged_in#112, _c12#29 AS num_compromised#113, _c13#30 AS root_shell#114, _c14#31 AS su_attempted#115, _c15#32 AS num_root#116, _c16#33 AS num_file_creations#117, _c17#34 AS num_shells#118, _c18#35 AS num_access_files#119, _c19#36 AS num_outbound_cmds#120, _c20#37 AS is_host_login#121, _c21#38 AS is_guest_login#122, _c22#39 AS count#123, _c23#40 AS srv_count#124, ... 18 more fields]\n         +- Relation [_c0#17,_c1#18,_c2#19,_c3#20,_c4#21,_c5#22,_c6#23,_c7#24,_c8#25,_c9#26,_c10#27,_c11#28,_c12#29,_c13#30,_c14#31,_c15#32,_c16#33,_c17#34,_c18#35,_c19#36,_c20#37,_c21#38,_c22#39,_c23#40,... 18 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "threshold = clustered.select(\"cluster\", \"scaledFeatureVector\").withColumn(\"dist_value\", dist(col(\"cluster\"), col(\"scaledFeatureVector\"))).orderBy(col(\"dist_value\").desc()).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9510f2-2ccb-4d3f-af60-8a9d058eb54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
